{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from queue import Queue\n",
    "from queue import Empty\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "try:\n",
    "    sys.path.append(\n",
    "        glob.glob(\"../carla/dist/carla-*%d.%d-%s.egg\"\n",
    "            % (sys.version_info.major,sys.version_info.minor,\"win-amd64\" if os.name == \"nt\" else \"linux-x86_64\",))[0])\n",
    "except IndexError:\n",
    "    pass\n",
    "\n",
    "import carla\n",
    "\n",
    "CAMERA_POSITION =  [1.3, 0.0, 2.3]\n",
    "CAMERA_HEIGHT = 480\n",
    "CAMERA_WIDTH = 960\n",
    "CAMERA_FOV = 120\n",
    "CAMERA_LEFT = -60.0\n",
    "CAMERA_RIGHT = 60.0\n",
    "\n",
    "# IMAGE\n",
    "IMAGE_WIDTH = 320\n",
    "IMAGE_RESOLUTION = (160, 704)\n",
    "\n",
    "COMBINED_IMAGE_SHAPE = (160, 704, 3)\n",
    "\n",
    "MODEL_INPUT_SIZE = (10,) + COMBINED_IMAGE_SHAPE\n",
    "\n",
    "MODEL_OUTPUT_SIZE = (5,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_img(image, scale=1, start_x=0, crop_x=None, start_y=0, crop_y=None):\n",
    "    i = np.array(image.raw_data)\n",
    "    i2 = i.reshape((image.height, image.width, 4))\n",
    "    i3 = i2[:, :, :3]\n",
    "    \"\"\" cv2.imshow(\"\", i3)\n",
    "    cv2.waitKey(1) \"\"\"\n",
    "    #i3/255.0\n",
    "    i3 = Image.fromarray(i3)\n",
    "    (width, height) = (i3.width // scale, i3.height // scale)\n",
    "    if crop_x is None:\n",
    "        crop_x = width\n",
    "    if crop_y is None:\n",
    "        crop_y = height\n",
    "        \n",
    "    i4 = np.asarray(i3)\n",
    "    cropped_image = i4[start_y:start_y+crop_y, start_x:start_x+crop_x]\n",
    "    return cropped_image#/255.0\n",
    "\n",
    "def sensor_callback(sensor_data, sensor_queue, sensor_name):\n",
    "    processed_img = process_img(sensor_data, 1, IMAGE_WIDTH, IMAGE_WIDTH, IMAGE_RESOLUTION[0], IMAGE_RESOLUTION[0])\n",
    "    sensor_queue.put((processed_img, sensor_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "sensor_list = []\n",
    "def camera_rgb_install():\n",
    "    camera_positions = [\n",
    "        carla.Transform(\n",
    "            carla.Location(x=CAMERA_POSITION[0], z=CAMERA_POSITION[2]),\n",
    "            carla.Rotation(yaw=CAMERA_LEFT),\n",
    "        ),  # Left\n",
    "        carla.Transform(\n",
    "            carla.Location(x=CAMERA_POSITION[0], z=CAMERA_POSITION[2])\n",
    "        ),  # Center\n",
    "        carla.Transform(\n",
    "            carla.Location(x=CAMERA_POSITION[0], z=CAMERA_POSITION[2]),\n",
    "            carla.Rotation(yaw=CAMERA_RIGHT),\n",
    "        ),\n",
    "    ]  # Right\n",
    "\n",
    "    cameras = []\n",
    "    for camera_transform in camera_positions:\n",
    "        camera_blueprint = world.get_blueprint_library().find(\"sensor.camera.rgb\")\n",
    "        camera_blueprint.set_attribute(\"image_size_x\", str(CAMERA_WIDTH))\n",
    "        camera_blueprint.set_attribute(\"image_size_y\", str(CAMERA_HEIGHT))\n",
    "        camera_blueprint.set_attribute(\"fov\", str(CAMERA_FOV))\n",
    "        #camera_blueprint.set_attribute(\"sensor_tick\", \"0.1\")\n",
    "        camera = world.spawn_actor(\n",
    "            camera_blueprint, camera_transform, attach_to=ego_vehicle\n",
    "        )\n",
    "        actor_list.append(camera)\n",
    "        cameras.append(camera)\n",
    "        sensor_list.append(camera)\n",
    "    return cameras\n",
    "\n",
    "\n",
    "actor_list = []\n",
    "\n",
    "client = carla.Client(\"localhost\", 2000)\n",
    "client.set_timeout(3.0)\n",
    "\n",
    "# Once we have a client we can retrieve the world that is currently running.\n",
    "world = client.get_world()\n",
    "\n",
    "try:\n",
    "    # We need to save the settings to be able to recover them at the end\n",
    "    # of the script to leave the server in the same state that we found it.\n",
    "    original_settings = world.get_settings()\n",
    "    settings = world.get_settings()\n",
    "\n",
    "    # We set CARLA syncronous mode\n",
    "    settings.fixed_delta_seconds = 0.05  # Fixed time-step == 10 FPS (1 / 0,1)\n",
    "    settings.synchronous_mode = True\n",
    "    world.apply_settings(settings)\n",
    "\n",
    "    sensor_queue = Queue()\n",
    "\n",
    "    # Retrieve the spectator object\n",
    "    spectator = world.get_spectator()\n",
    "\n",
    "    # Get the location and rotation of the spectator through its transform\n",
    "    transform = spectator.get_transform()\n",
    "\n",
    "    location = transform.location\n",
    "    rotation = transform.rotation\n",
    "\n",
    "    blueprint_library = world.get_blueprint_library()\n",
    "\n",
    "    # Set the spectator with an empty transform\n",
    "    # spectator.set_transform(carla.Transform())\n",
    "\n",
    "    # This will set the spectator at the origin of the map, with 0 degrees\n",
    "    # pitch, yaw and roll - a good way to orient yourself in the map\n",
    "\n",
    "    vehicle_blueprints = blueprint_library.filter(\"*vehicle*\")\n",
    "\n",
    "    spawn_points = world.get_map().get_spawn_points()\n",
    "\n",
    "    ego_vehicle = world.spawn_actor(\n",
    "        blueprint_library.filter(\"etron\")[0], random.choice(spawn_points)\n",
    "    )\n",
    "    actor_list.append(ego_vehicle)\n",
    "    ego_transform = ego_vehicle.get_transform()\n",
    "\n",
    "    spectator_location = ego_transform.location + carla.Location(0, 0, 100)\n",
    "    spectator_rotation = ego_transform.rotation\n",
    "    spectator.set_transform(\n",
    "        carla.Transform(\n",
    "            spectator_location,\n",
    "            spectator_rotation,\n",
    "        )\n",
    "    )\n",
    "    spectator_rotation.pitch = -90\n",
    "\n",
    "    \"\"\" for i in range(0, 50):\n",
    "        npc_vehicle = world.try_spawn_actor(\n",
    "            random.choice(vehicle_blueprints), random.choice(spawn_points)\n",
    "        )\n",
    "        if npc_vehicle is not None:\n",
    "            # npc_vehicle.set_autopilot(True)\n",
    "            actor_list.append(npc_vehicle) \"\"\"\n",
    "\n",
    "    \"\"\" postavljanje senzora na auto \"\"\"\n",
    "    # kamera RGB\n",
    "    camera_list = camera_rgb_install()\n",
    "\n",
    "    ego_vehicle.get_control().steer = 0.0\n",
    "    ego_vehicle.get_control().throttle = 0.0\n",
    "    ego_vehicle.get_control().brake = 0.0\n",
    "    ego_vehicle.get_control().hand_brake = False\n",
    "    ego_vehicle.get_control().reverse = False\n",
    "\n",
    "    \"\"\" while True:\n",
    "        world.tick()\n",
    "\n",
    "        ego_vehicle.get_control().throttle = 0.3\n",
    "\n",
    "\n",
    "        shape = (3, IMAGE_WIDTH, IMAGE_RESOLUTION[0], 3)\n",
    "        image_queue = np.empty((10, 160, 704, 3), dtype=np.float32)\n",
    "\n",
    "        camera_images_np = np.empty(shape, dtype=np.float32)\n",
    "        camera_images = []\n",
    "\n",
    "        for camera in camera_list:\n",
    "            camera.listen(lambda data: sensor_callback(data, sensor_queue, \"camera%d\" % camera_list.index(camera)))\n",
    "\n",
    "        try:\n",
    "            for _ in range(len(sensor_list)):\n",
    "                s_frame = sensor_queue.get(True, 1.0)\n",
    "                print(\"    Frame: %d   Sensor: %s\" % (s_frame[0], s_frame[1]))\n",
    "\n",
    "        except Empty:\n",
    "            print(\"    Some of the sensor information is missed\") \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    seconds = 3\n",
    "    image_queue = np.empty((10, 160, 704, 3), dtype=np.float32)\n",
    "    for sec in range(seconds):\n",
    "\n",
    "        # Synchronize the CARLA world\n",
    "        world.tick()\n",
    "\n",
    "        ego_vehicle.get_control().throttle = 0.3\n",
    "\n",
    "\n",
    "        shape = (3, IMAGE_WIDTH, IMAGE_RESOLUTION[0], 3)\n",
    "\n",
    "        camera_images_np = np.empty(shape, dtype=np.float32)\n",
    "        camera_images = []\n",
    "\n",
    "        camera_images_np = np.empty(shape, dtype=np.float32)\n",
    "        combined_image = np.empty(COMBINED_IMAGE_SHAPE, dtype=np.float32)\n",
    "        camera_images = []\n",
    "\n",
    "        for camera in camera_list:\n",
    "            camera.listen(lambda data: sensor_callback(data, sensor_queue, \"camera%d\" % camera_list.index(camera)))\n",
    "\n",
    "        while len(sensor_list) != 3:\n",
    "            pass\n",
    "        try:\n",
    "            for _ in range(len(sensor_list)):\n",
    "                s_frame = sensor_queue.get(True, 1.0)\n",
    "                print(\"    Frame: %d   Sensor: %s\" % (s_frame[0], s_frame[1]))\n",
    "\n",
    "        except Empty:\n",
    "            print(\"    Some of the sensor information is missed\") \n",
    "\n",
    "        \"\"\" for camera in camera_list:\n",
    "            camera.listen(lambda image: camera_images.append(process_img(image, 1, IMAGE_WIDTH, IMAGE_WIDTH, IMAGE_RESOLUTION[0], IMAGE_RESOLUTION[0])))\n",
    "        \n",
    "        time.sleep(0.1)\n",
    "\n",
    "        print(\"Camera Images before: \", len(camera_images))\n",
    "        time.sleep(0.1)\n",
    "        print(\"Camera Images after: \", len(camera_images))\n",
    "        while len(camera_images) < len(camera_list):\n",
    "            continue\n",
    "\n",
    "        combined_image = np.concatenate(np.array(camera_images), axis=1)\n",
    "        cv2.imwrite(\"images/combined_image.png\", combined_image)\n",
    "\n",
    "        np.append(image_queue,combined_image)\n",
    "        print(\"shape: \", image_queue.shape)\n",
    "        cv2.imwrite(\"images/image_queue.png\", image_queue[0]) \"\"\"\n",
    "\n",
    "        if (len(image_queue) == 10):\n",
    "            print(\"model predict\")\n",
    "\n",
    "       \n",
    "   \n",
    "\n",
    "finally:\n",
    "    world.apply_settings(original_settings)\n",
    "    for actor in actor_list:\n",
    "        actor.destroy()\n",
    "    print(\"done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-directml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
